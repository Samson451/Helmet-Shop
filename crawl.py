import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import json
import time
import re
import os
from pathlib import Path

# C·∫•u h√¨nh
BASE_URL = "https://bikersaigon.net"
DELAY = 1.5  # Th·ªùi gian ch·ªù gi·ªØa c√°c request (gi√¢y)
HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
IMAGE_ROOT = "images" 
MAX_RETRIES = 3

def get_soup(url, retry=0):
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        return BeautifulSoup(response.text, 'html.parser')
    except Exception as e:
        if retry < MAX_RETRIES:
            print(f"‚ö†Ô∏è Th·ª≠ l·∫°i ({retry+1}/{MAX_RETRIES}) cho {url}")
            time.sleep(DELAY * 2)
            return get_soup(url, retry + 1)
        print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ l·∫•y {url} sau {MAX_RETRIES} l·∫ßn th·ª≠: {str(e)}")
        return None

def scrape_menu():
    print("üîç ƒêang thu th·∫≠p danh m·ª•c ch√≠nh s·∫£n ph·∫©m...")
    soup = get_soup(BASE_URL)
    if not soup:
        return []

    # T√¨m mega_menu b·∫±ng id
    mega_menu = soup.find('ul', id='mega_menu')
    if not mega_menu:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y mega menu!")
        return []

    categories = []
    seen_urls = set()

    # Ch·ªâ l·∫•y c√°c li tr·ª±c ti·∫øp trong mega_menu
    for li in mega_menu.find_all('li', recursive=False):
        # T√¨m th·∫ª a ƒë·∫ßu ti√™n trong li
        link = li.find('a', href=True)
        if not link:
            continue
            
        href = link.get('href')
        if not href or href == '#' or href.startswith('javascript:'):
            continue
            
        # L·∫•y t√™n danh m·ª•c t·ª´ span c√≥ class menu-image-title ho·∫∑c menu-image-title-after
        name_span = link.find('span', class_=['menu-image-title', 'menu-image-title-after'])
        if name_span:
            category_name = name_span.get_text(strip=True)
        else:
            # N·∫øu kh√¥ng c√≥ span, l·∫•y text tr·ª±c ti·∫øp t·ª´ th·∫ª a
            category_name = link.get_text(strip=True)
            # L√†m s·∫°ch t√™n
            category_name = re.sub(r'\s+', ' ', category_name).strip()
            
        # B·ªè qua n·∫øu kh√¥ng c√≥ t√™n
        if not category_name:
            continue
            
        # T·∫°o URL tuy·ªát ƒë·ªëi
        category_url = urljoin(BASE_URL, href)
        
        # Ki·ªÉm tra URL h·ª£p l·ªá v√† ch∆∞a t·ªìn t·∫°i
        if category_url not in seen_urls:
            seen_urls.add(category_url)
            categories.append({
                "type_name": category_name,
                "url": category_url,
                "type_id": len(categories) + 1
            })
            print(f"‚úÖ Ph√°t hi·ªán danh m·ª•c ch√≠nh: {category_name}")

    return categories

def get_total_pages(soup):
    """X√°c ƒë·ªãnh t·ªïng s·ªë trang t·ª´ ph√¢n trang"""
    pagination = soup.select_one('.page-numbers.nav-pagination')
    if not pagination:
        return 1
    
    # T√¨m t·∫•t c·∫£ c√°c trang
    page_links = pagination.find_all('a', class_='page-number')
    max_page = 1
    
    for link in page_links:
        try:
            page_num = int(link.text)
            if page_num > max_page:
                max_page = page_num
        except ValueError:
            pass
    
    # Ki·ªÉm tra trang cu·ªëi
    last_page_link = pagination.find('a', class_='next')
    if last_page_link and 'href' in last_page_link.attrs:
        last_url = last_page_link['href']
        if '/page/' in last_url:
            try:
                page_num = int(last_url.split('/page/')[1].split('/')[0])
                if page_num > max_page:
                    max_page = page_num
            except:
                pass
    
    return max_page

def scrape_category(category, crawled_urls):
    """Thu th·∫≠p s·∫£n ph·∫©m t·ª´ m·ªôt danh m·ª•c, ki·ªÉm tra tr√πng l·∫∑p"""
    print(f"\nüìÇ ƒêang x·ª≠ l√Ω danh m·ª•c: {category['type_name']}")
    products = []
    page = 1
    total_pages = 1
    
    while True:
        # T·∫°o URL ph√¢n trang
        if page > 1:
            page_url = f"{category['url']}page/{page}/"
        else:
            page_url = category['url']
        
        print(f"üìÑ Trang {page}: {page_url}")
        
        soup = get_soup(page_url)
        if not soup:
            print("‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫£i trang, b·ªè qua")
            break
        
        # L·∫•y t·ªïng s·ªë trang (ch·ªâ l·∫ßn ƒë·∫ßu)
        if page == 1:
            total_pages = get_total_pages(soup)
            print(f"‚ÑπÔ∏è T·ªïng s·ªë trang: {total_pages}")
        
        # T√¨m s·∫£n ph·∫©m
        product_items = soup.select('div.product-small')
        if not product_items:
            print("‚ÑπÔ∏è Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m tr√™n trang n√†y")
            break
        
        for item in product_items:
            try:
                product_link = item.find('a', class_='woocommerce-LoopProduct-link')
                if not product_link or not product_link.get('href'):
                    continue
                    
                product_url = urljoin(page_url, product_link['href'])
                
                # Ki·ªÉm tra n·∫øu s·∫£n ph·∫©m ƒë√£ ƒë∆∞·ª£c crawl
                if product_url in crawled_urls:
                    print(f"‚è© B·ªè qua s·∫£n ph·∫©m ƒë√£ thu th·∫≠p: {product_url}")
                    continue
                    
                # ƒê√°nh d·∫•u URL ƒë√£ crawl
                crawled_urls.add(product_url)
                
                time.sleep(DELAY)
                product_id = len(products) + 1 
                product_data = scrape_product(product_url, category, product_id)
                if product_data:
                    products.append(product_data)
                    print(f"üõí ƒê√£ thu th·∫≠p: {product_data['product_name']}")
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω s·∫£n ph·∫©m: {str(e)}")
        
        # Ki·ªÉm tra n·∫øu ƒë√£ h·∫øt trang
        if page >= total_pages:
            break
            
        page += 1
        time.sleep(DELAY)
    
    print(f"‚úÖ Ho√†n th√†nh danh m·ª•c: {len(products)} s·∫£n ph·∫©m")
    return products

def scrape_product(product_url, category):
    """Thu th·∫≠p chi ti·∫øt s·∫£n ph·∫©m"""
    soup = get_soup(product_url)
    if not soup:
        return None

    # L·∫•y t√™n s·∫£n ph·∫©m
    name_tag = soup.find('h1', class_='product-title')
    if not name_tag:
        return None
    product_name = name_tag.get_text(strip=True)
    
    # L·∫•y gi√°
    price_tag = soup.find('p', class_='price')
    price = price_tag.get_text(strip=True) if price_tag else "Li√™n h·ªá"
    
    # L·∫•y m√¥ t·∫£
    desc_tag = soup.find('div', class_='product-short-description')
    description = desc_tag.get_text(strip=True) if desc_tag else ""
    
    # L·∫•y ·∫£nh ch√≠nh
    main_img_tag = soup.select_one('div.product-images img')
    main_img_url = ""
    if main_img_tag:
        main_img_url = main_img_tag.get('data-src') or main_img_tag.get('src')
        if main_img_url and main_img_url.startswith('//'):
            main_img_url = 'https:' + main_img_url
    
    # T·∫°o th∆∞ m·ª•c l∆∞u ·∫£nh (trong th∆∞ m·ª•c "images")
    safe_name = re.sub(r'[\\/*?:"<>|]', '', product_name).strip()
    product_dir = Path(IMAGE_ROOT) / safe_name
    product_dir.mkdir(parents=True, exist_ok=True)
    
    # T·∫£i ·∫£nh ch√≠nh
    main_img_path = ""
    if main_img_url and not main_img_url.startswith('data:'):
        try:
            # X√°c ƒë·ªãnh ph·∫ßn m·ªü r·ªông c·ªßa ·∫£nh
            img_ext = os.path.splitext(main_img_url)[1]
            if not img_ext or len(img_ext) > 5:  # N·∫øu kh√¥ng c√≥ ho·∫∑c ph·∫ßn m·ªü r·ªông qu√° d√†i
                img_ext = ".jpg"
                
            img_name = "main" + img_ext
            img_path = product_dir / img_name
            response = requests.get(main_img_url, headers=HEADERS, stream=True, timeout=10)
            if response.status_code == 200:
                with open(img_path, 'wb') as f:
                    for chunk in response.iter_content(1024):
                        f.write(chunk)
                # S·ª¨A: S·ª≠ d·ª•ng d·∫•u \ v√† th√™m "images\" v√†o ƒë∆∞·ªùng d·∫´n
                main_img_path = f"images\\{safe_name}\\{img_name}"
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng t·∫£i ƒë∆∞·ª£c ·∫£nh ch√≠nh: {e}")
    
    # L·∫•y ·∫£nh gallery
    gallery_images = []
    gallery = soup.find('div', class_='product-thumbnails')
    if gallery:
        for i, img in enumerate(gallery.find_all('img'), 1):
            img_url = img.get('data-src') or img.get('src')
            if img_url and not img_url.startswith('data:'):
                if img_url.startswith('//'):
                    img_url = 'https:' + img_url
                try:
                    # X√°c ƒë·ªãnh ph·∫ßn m·ªü r·ªông c·ªßa ·∫£nh
                    img_ext = os.path.splitext(img_url)[1]
                    if not img_ext or len(img_ext) > 5:  # N·∫øu kh√¥ng c√≥ ho·∫∑c ph·∫ßn m·ªü r·ªông qu√° d√†i
                        img_ext = ".jpg"
                        
                    img_name = f"gallery_{i}{img_ext}"
                    img_path = product_dir / img_name
                    response = requests.get(img_url, headers=HEADERS, stream=True, timeout=10)
                    if response.status_code == 200:
                        with open(img_path, 'wb') as f:
                            for chunk in response.iter_content(1024):
                                f.write(chunk)
                        # S·ª¨A: S·ª≠ d·ª•ng d·∫•u \ v√† th√™m "images\" v√†o ƒë∆∞·ªùng d·∫´n
                        gallery_images.append(f"images\\{safe_name}\\{img_name}")
                except Exception as e:
                    print(f"‚ö†Ô∏è Kh√¥ng t·∫£i ƒë∆∞·ª£c ·∫£nh gallery: {e}")
    
    return {
        "type_id": category['type_id'],
        "type_name": category['type_name'],
        "product_name": product_name,
        "price": price,
        "description": description,
        "main_image": main_img_path,
        "gallery_images": gallery_images,
        "product_url": product_url
    }

def save_data(data, filename):
    """L∆∞u d·ªØ li·ªáu v√†o file JSON"""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"üíæ ƒê√£ l∆∞u {len(data)} m·ª•c v√†o {filename}")
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi l∆∞u file {filename}: {str(e)}")

def main():
    start_time = time.time()
    
    # T·∫°o th∆∞ m·ª•c h√¨nh ·∫£nh
    Path(IMAGE_ROOT).mkdir(exist_ok=True)
    
    # B∆∞·ªõc 1: Thu th·∫≠p danh m·ª•c ch√≠nh
    categories = scrape_menu()
    
    # T·∫°o danh s√°ch ch·ªâ ch·ª©a id v√† t√™n ƒë·ªÉ l∆∞u v√†o JSON
    categories_for_json = [
        {"type_id": cat["type_id"], "type_name": cat["type_name"]} 
        for cat in categories
    ]
    save_data(categories_for_json, "product_types.json")
    
    if not categories:
        print("‚ùå Kh√¥ng t√¨m th·∫•y danh m·ª•c s·∫£n ph·∫©m!")
        return
    
    # B∆∞·ªõc 2: Thu th·∫≠p s·∫£n ph·∫©m t·ª´ng danh m·ª•c
    all_products = []
    crawled_urls = set()  # T·∫≠p h·ª£p l∆∞u c√°c URL s·∫£n ph·∫©m ƒë√£ crawl
    
    for category in categories:
        products = scrape_category(category, crawled_urls)
        all_products.extend(products)
        save_data(all_products, "products.json")  # L∆∞u t·∫°m sau m·ªói danh m·ª•c
    
    # B∆∞·ªõc 3: L∆∞u k·∫øt qu·∫£ cu·ªëi c√πng
    save_data(all_products, "products.json")
    
    # Th·ªëng k√™
    total_time = (time.time() - start_time) / 60
    print(f"\n‚úÖ Ho√†n th√†nh trong {total_time:.2f} ph√∫t")
    print(f"üìä T·ªïng s·ªë danh m·ª•c ch√≠nh: {len(categories)}")
    print(f"üõçÔ∏è T·ªïng s·ªë s·∫£n ph·∫©m: {len(all_products)}")
    print(f"üìÅ Th∆∞ m·ª•c ·∫£nh: {IMAGE_ROOT}")

if __name__ == "__main__":
    main()